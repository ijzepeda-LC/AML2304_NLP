{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c64c73",
   "metadata": {},
   "source": [
    "# Ivan zepeda\n",
    "#### C0883949"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f1834",
   "metadata": {},
   "source": [
    "<img src=\"https://pythonspot.com/wp-content/uploads/2016/08/nltk-speech-codes.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa645a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('found', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('coach', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('bed', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('new', 'JJ'),\n",
       " ('apartment', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "mystring= \" John found a new coach and a new bed in his new apartment\"\n",
    "output = TextBlob (mystring)\n",
    "output.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35873a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "reg_exp= \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "rp = nltk.chunk.RegexpParser(reg_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cef144ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  John/NNP\n",
      "  found/VBD\n",
      "  (NP a/DT new/JJ coach/NN)\n",
      "  and/CC\n",
      "  (NP a/DT new/JJ bed/NN)\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  (NP new/JJ apartment/NN))\n"
     ]
    }
   ],
   "source": [
    "output_2=rp.parse(output.tags)\n",
    "print(output_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f1cf40",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1548291719.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Stop! Running the following line will open a new window, looping python on it, forcing to close it and loose the session\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Stop! Running the following line will open a new window, looping python on it, forcing to close it and loose the session\n",
    "output.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d11b70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  John/NNP\n",
      "  found/VBD\n",
      "  a/DT\n",
      "  (NP new/JJ coach/NN)\n",
      "  and/CC\n",
      "  a/DT\n",
      "  (NP new/JJ bed/NN)\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  (NP new/JJ apartment/NN))\n"
     ]
    }
   ],
   "source": [
    "# Possible to get two different chunks\n",
    "reg_exp= \"NP: {<JJ>*<NN> | <PRP$>?<DT>*<NN>}\"\n",
    "rp = nltk.chunk.RegexpParser(reg_exp)\n",
    "output_2=rp.parse(output.tags)\n",
    "print(output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad0809",
   "metadata": {},
   "source": [
    "## 7.2 CHINKING\n",
    "Removing a chunk from a chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21426cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Th', 'NNP'),\n",
       " ('little', 'JJ'),\n",
       " ('yellow', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('barked', 'VBD'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('cat', 'NN')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystring2= \"Th little yellow dog barked at the cat\"\n",
    "# Set string on textblob\n",
    "tb = TextBlob(mystring2)\n",
    "#print the tags of the textblobed\n",
    "tb.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "733b6a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Th/NNP little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "regexp=r\"\"\" NP:\n",
    "{<.*>+}      # Chunk everything\n",
    "}<VBD|IN>+{  # Chink Sequence of VBD and IN\n",
    "\"\"\"\n",
    "\n",
    "rp = nltk.chunk.RegexpParser(regexp)\n",
    "out= rp.parse(tb.tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72a44c",
   "metadata": {},
   "source": [
    "#  \n",
    "#  \n",
    "#  \n",
    "#  Module 8\n",
    "Classifiers using pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bcc2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d50b25ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n"
     ]
    }
   ],
   "source": [
    "news= fetch_20newsgroups(subset='all')\n",
    "print(len(news.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5e65ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(news.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d6f9da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(news.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b810149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "778f8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(classifier, X=news.data, y=news.target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=48)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print('classifier accuracy is {}'.format(classifier.score(X_test, y_test)))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e6fd68",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c071af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a25f9a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier accuracy is 0.8535653650254669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIRST PIPELINE\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "trial1 = Pipeline([(\"vectorizer\", TfidfVectorizer()), ('classifier',MultinomialNB())])\n",
    "t1=train_test(trial1)\n",
    "t1\n",
    "# 0.8535653650354669 > original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "48764ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9afb2f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier accuracy is 0.8828522920203735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vetorizer',\n",
       "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SECOND PIPELINE\n",
    "from nltk.corpus import stopwords\n",
    "trial2 = Pipeline([(\"vetorizer\", TfidfVectorizer(stop_words=en_stopwords)),\n",
    "                  ('classifier',MultinomialNB())])\n",
    "t2=train_test(trial2)\n",
    "t2\n",
    "# 0.8828522920203735 > original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ee03b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier accuracy is 0.8926146010186757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('classifier', MultinomialNB(alpha=0.5))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third PIPELINE\n",
    "trail3 = Pipeline([('vectorizer',TfidfVectorizer(stop_words=en_stopwords)),\n",
    "                  ('classifier',MultinomialNB(alpha=0.5))], verbose=False)\n",
    "t3=train_test(trail3) \n",
    "t3\n",
    "# 0.91553480475382 > Original\n",
    "# 0.8926146010186757 > My attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc3ef764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier accuracy is 0.9276315789473685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(min_df=5,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
       "                ('classifier', LinearSVC())])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FOURTH PIPELINE\n",
    "from sklearn import svm\n",
    "import string\n",
    "trail4 = Pipeline([('vectorizer',TfidfVectorizer(stop_words=stopwords.words('english')+list(string.punctuation), min_df=5)),\n",
    "                  ('classifier', svm.LinearSVC())])\n",
    "t4=train_test(trail4)\n",
    "t4\n",
    "# 00.9276315789473685 > original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8baf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
