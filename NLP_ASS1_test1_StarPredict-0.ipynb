{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aca3a6b",
   "metadata": {},
   "source": [
    "# After Original data\n",
    "el procesamiento de data parece correcto y estable como esta.  \n",
    "ahora seria cmabairle el modelo, en lugar de usar un ridge, usar otro.  \n",
    "o uno de classification, ya que usa un regression.  \n",
    "Debe tener la discusion de las metricas, y porque si o porque no\n",
    "\n",
    ">> Python have templates reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe1ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e28d245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pale imitation of better films</td>\n",
       "      <td>Three words: \"Cool Hand Luke.\"  Same film, don...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Didactic and overlong</td>\n",
       "      <td>Another one of those overlong morally right-on...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mediocre people reward mediocre cinema.</td>\n",
       "      <td>It bugs me that this movie is rated so high- n...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The plain &amp; simple truth. It doesn\\'t deserve ...</td>\n",
       "      <td>just read the title. Tough I think it's a pret...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not the greatest movie of all time</td>\n",
       "      <td>Shawshank is nothing more than a fairy tale.  ...</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                     Pale imitation of better films   \n",
       "1                              Didactic and overlong   \n",
       "2            Mediocre people reward mediocre cinema.   \n",
       "3  The plain & simple truth. It doesn\\'t deserve ...   \n",
       "4                 Not the greatest movie of all time   \n",
       "\n",
       "                                             content rating  \n",
       "0  Three words: \"Cool Hand Luke.\"  Same film, don...   1/10  \n",
       "1  Another one of those overlong morally right-on...   1/10  \n",
       "2  It bugs me that this movie is rated so high- n...   1/10  \n",
       "3  just read the title. Tough I think it's a pret...   1/10  \n",
       "4  Shawshank is nothing more than a fairy tale.  ...   1/10  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw=pd.read_csv('reviews.csv')\n",
    "df = df_raw[['title','content','rating']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26ed8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1/10', '2/10', '3/10', '4/10', '5/10', '6/10', '7/10', '8/10',\n",
       "       '9/10', '10/10'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(df['rating'].unique())\n",
    "df['rating'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa5aed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pale imitation of better films</td>\n",
       "      <td>Three words: \"Cool Hand Luke.\"  Same film, don...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Didactic and overlong</td>\n",
       "      <td>Another one of those overlong morally right-on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mediocre people reward mediocre cinema.</td>\n",
       "      <td>It bugs me that this movie is rated so high- n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The plain &amp; simple truth. It doesn\\'t deserve ...</td>\n",
       "      <td>just read the title. Tough I think it's a pret...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not the greatest movie of all time</td>\n",
       "      <td>Shawshank is nothing more than a fairy tale.  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20306</th>\n",
       "      <td>extremely scary</td>\n",
       "      <td>peter Lorre is the best child serial killer ci...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20307</th>\n",
       "      <td>Classic Mystery</td>\n",
       "      <td>I thoroughly enjoyed watching this film. Seein...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20308</th>\n",
       "      <td>One of the most influential films ever, almost...</td>\n",
       "      <td>One of the most influential films ever, almost...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20309</th>\n",
       "      <td>M a few thoughts</td>\n",
       "      <td>I will not take time to praise the film, as ot...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20310</th>\n",
       "      <td>Still a remarkably powerful film.</td>\n",
       "      <td>If you found Todd Solendz's recent film \"Happi...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20311 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0                         Pale imitation of better films   \n",
       "1                                  Didactic and overlong   \n",
       "2                Mediocre people reward mediocre cinema.   \n",
       "3      The plain & simple truth. It doesn\\'t deserve ...   \n",
       "4                     Not the greatest movie of all time   \n",
       "...                                                  ...   \n",
       "20306                                    extremely scary   \n",
       "20307                                    Classic Mystery   \n",
       "20308  One of the most influential films ever, almost...   \n",
       "20309                                   M a few thoughts   \n",
       "20310                  Still a remarkably powerful film.   \n",
       "\n",
       "                                                 content  rating  \n",
       "0      Three words: \"Cool Hand Luke.\"  Same film, don...       1  \n",
       "1      Another one of those overlong morally right-on...       1  \n",
       "2      It bugs me that this movie is rated so high- n...       1  \n",
       "3      just read the title. Tough I think it's a pret...       1  \n",
       "4      Shawshank is nothing more than a fairy tale.  ...       1  \n",
       "...                                                  ...     ...  \n",
       "20306  peter Lorre is the best child serial killer ci...      10  \n",
       "20307  I thoroughly enjoyed watching this film. Seein...      10  \n",
       "20308  One of the most influential films ever, almost...      10  \n",
       "20309  I will not take time to praise the film, as ot...      10  \n",
       "20310  If you found Todd Solendz's recent film \"Happi...      10  \n",
       "\n",
       "[20311 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = df[~df['rating'].isna()]\n",
    "df['rating']=[int(x.split('/')[0]) for x in df['rating']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4510a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "content    0\n",
       "rating     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b155edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     2496\n",
       "8     2493\n",
       "10    2492\n",
       "7     2432\n",
       "6     2202\n",
       "5     1925\n",
       "1     1790\n",
       "4     1612\n",
       "3     1489\n",
       "2     1380\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936591b",
   "metadata": {},
   "source": [
    "# Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d9a7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\n','. ')#.replace('','').replace('','').replace('','')\n",
    "    return text\n",
    "\n",
    "def remove_punct(text, lower=False):\n",
    "    if(type(text)==str):\n",
    "        text = text.replace('\\n','. ')#.replace('','').replace('','').replace('','')\n",
    "        text = text.replace(',',' ').replace('!','').replace('?',' ')\n",
    "        text = text.replace('.',' ').replace('#','').replace('$',' ')\n",
    "        text = text.replace('^',' ').replace('&','and').replace(';',' ')\n",
    "        text = text.replace('  ',' ')\n",
    "        return text\n",
    "    elif(type(text)==list):\n",
    "        _words=[]\n",
    "        for w in text:\n",
    "            if w.isalnum():\n",
    "                _words.append(w if not lower else w.lower())\n",
    "#         print('remove_punct',len(_words))\n",
    "        return _words\n",
    "\n",
    "def get_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def get_tokens(text):\n",
    "    words =[]\n",
    "    for w in word_tokenize(text):\n",
    "        if w.isalnum():\n",
    "            words.append(w)\n",
    "    return words\n",
    "    \n",
    "\n",
    "def remove_stopwords(words):\n",
    "    just_words=[]\n",
    "    for word in words:\n",
    "        if word.lower() not in stopword:\n",
    "            just_words.append(word)\n",
    "#     print('just_words',len(just_words))\n",
    "    return just_words\n",
    "    \n",
    "    \n",
    "def lemmatize(text):\n",
    "    record_lemmatized = [lemmatizer.lemmatize(token) for token in text]\n",
    "    return record_lemmatized\n",
    "    \n",
    "    \n",
    "def list_to_string(ls):\n",
    "    return \" \".join(ls)\n",
    "    \n",
    "# clean words: boooook\n",
    "# Remove non stop words of 2 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8201d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c9d4467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Three words: \"Cool Hand Luke.\"  Same film, done better, done earlier.  For that matter, is this film any better than other Steven King \"novelettes\" such as \"Stand By Me\"? All in all, it probably ranks a 6 or a 7, but since people on this site have lost their minds as regards this film, I give it a 1 in one man\\'s attempt at sanity.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43c0acb1",
   "metadata": {},
   "source": [
    "# dataset_tokens[0]\n",
    "# dataset_nostopwords[0]\n",
    "# dataset_main_words[0]\n",
    "# record_lemmatized[0]\n",
    "# dataset_clean_text[0]\n",
    "\n",
    "_x=list_to_string(dataset_main_words[0])\n",
    "_=lemmatizer.lemmatize(_x)\n",
    "\n",
    "print(\">Text in tokens\\n\",dataset_main_words[0]) \n",
    "print(\">Text in string\\n\",_x)\n",
    "print(\">Lemmatized:\\n\",_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d09fe68",
   "metadata": {},
   "source": [
    "_=lemmatizer.lemmatize(\"loving\",'v')\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76478f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning approach 1:\n",
    "    # tokenize everything, remove stop words, lower all, remove punctutation\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# set tokens each description\n",
    "dataset_tokens= [ get_tokens(x) for x in df['content']] \n",
    "\n",
    "# remove stopwords\n",
    "dataset_nostopwords= [remove_stopwords(x) for x in dataset_tokens]\n",
    "\n",
    "# remove punctuation\n",
    "dataset_main_words= [remove_punct(x, lower=False) for x in dataset_nostopwords ]\n",
    "\n",
    "# Lower all\n",
    "# dataset_main_words_lower=[x.lower() for x in dataset_main_words]\n",
    "\n",
    "# Lemmatize words :  IT requires POS\n",
    "# record_lemmatized = [lemmatizer.lemmatize(token) for token in dataset_main_words]\n",
    "\n",
    "\n",
    "# Back to string\n",
    "dataset_clean_text = [ list_to_string(x) for x in dataset_main_words]\n",
    "\n",
    "def preprocess_text_ap1(x):\n",
    "       # tokenize everything, remove stop words, lower all, remove punctutation\n",
    "    # set tokens each description\n",
    "    dataset_tokens= get_tokens(x)\n",
    "    # remove stopwords\n",
    "    dataset_nostopwords= remove_stopwords(dataset_tokens)\n",
    "    # remove punctuation\n",
    "    dataset_main_words= remove_punct(dataset_nostopwords, lower=False)\n",
    "    # Lower all\n",
    "    # dataset_main_words_lower=[x.lower() for x in dataset_main_words]\n",
    "    # Back to string\n",
    "    dataset_clean_text =  list_to_string(dataset_main_words)\n",
    "    return dataset_clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb66af",
   "metadata": {},
   "source": [
    "# METHOD 1 TFIDF\n",
    "\n",
    "compare to a manual method  \n",
    "separate the test-train in equal samples\n",
    "\n",
    "The training was giving a lot of 8+ ratings. I rerun this with stratify\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f7ebaa5",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have a list of movie reviews and their corresponding ratings\n",
    "reviews = df['content']\n",
    "ratings = df['rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split([df['content'],df['title']], ratings, test_size=0.2, random_state=42)\n",
    "# Split the data into training and testing sets, maintaining a similar ratio of ratings\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, ratings, test_size=0.2, random_state=42, stratify=ratings)\n",
    "\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "#Compare it to a manual method\n",
    "\n",
    "# Fit the vectorizer on the training reviews\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing reviews using the fitted vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a linear regression model\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# Train the linear regression model\n",
    "regressor.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict ratings for the testing reviews\n",
    "y_pred = regressor.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model using mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b039cee",
   "metadata": {},
   "source": [
    "# Original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "596ef6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9238615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d8e59be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.582455908824896\n",
      "Mean Absolute Error: 1.707997277161021\n",
      "Cross-Validated MSE: 4.431198160575902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pre-Processed:\\nMean Squared Error: 4.444501189186152\\nMean Absolute Error: 1.6877876810286814\\nCross-Validated MSE: 4.424840704022955'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v2, improved gpt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Assuming you have a list of movie reviews and their corresponding ratings\n",
    "if original:\n",
    "    reviews = df['content']\n",
    "    ratings = df['rating']\n",
    "else:\n",
    "    reviews = dataset_clean_text\n",
    "    ratings = df['rating']\n",
    "    \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(reviews, ratings, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, ratings, test_size=0.3, random_state=42, stratify=ratings)\n",
    "# \n",
    "# Create a TF-IDF vectorizer with preprocessing options\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=5000 # features made by term frequency\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the training reviews and transform the data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############# RIDGE  !! USe different model\n",
    "\n",
    "# Create a Ridge regression model with regularization\n",
    "regressor = Ridge(alpha=0.5)\n",
    "\n",
    "# Train the Ridge regression model\n",
    "regressor.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict ratings for the testing reviews\n",
    "y_pred = regressor.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model using mean squared error (MSE) and mean absolute error (MAE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Perform cross-validation to get a more robust performance estimate\n",
    "cv_scores = cross_val_score(regressor, X_train_tfidf, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_mse = -cv_scores.mean()\n",
    "print(\"Cross-Validated MSE:\", cv_mse)\n",
    "\n",
    "\"\"\"Pre-Processed:\n",
    "Mean Squared Error: 4.444501189186152\n",
    "Mean Absolute Error: 1.6877876810286814\n",
    "Cross-Validated MSE: 4.424840704022955\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "565a7e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 9.77 of 10\n"
     ]
    }
   ],
   "source": [
    "review_test=\"Imagine what you'd do, when you wake from a bad dream, to find you're held by four square walls, for as long as someone deems. No idea why you're trapped, what you've done, why you're kidnapped, just a ceaseless line of dumplings going down your gyoza hatch. Now some fifteen years have passed, every question has been asked, and you're suddenly set free, can start your own avenging spree. Before you do you need to feed, by eating something that's in need, so an octopus is ordered, and head first you cross the borders. But things aren't what they might seem, tied and tethered and undreamed, as the puppet master hovers, manipulates what you'll discover. Some films you cannot watch too often and this is one of the greatest pieces of cinematic brilliance ever created.\"\n",
    "rating_test=10\n",
    "X_test_tfidf = vectorizer.transform([review_test])\n",
    "predicted_rating = regressor.predict(X_test_tfidf)\n",
    "print(f\"Predicted: {'{:.2f}'.format(predicted_rating[0])} of {rating_test}\")\n",
    "# Sin procesar Predicted: 9.44 of 10\n",
    "# PRocesado: Predicted: 9.47 of 10\n",
    "#modded 8.5\n",
    "#mod2: Predicted: 9.25 of 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0501eb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 3.42 of 7\n"
     ]
    }
   ],
   "source": [
    "review_test=\"Was this a missed warning sign? The current climate in America is kind of portayed here. Though some would argue, that it doesn't depict all of America, not even just the one side who seems to have a glutton for punishment and seems to like to vote against their own interests. So while there is some relevance to current events, this go far out.Still if you feel uneasy watching this, it doesn't mean something is wrong with you. Quite the opposite is the case, everything is right with you. Quite ridiculous at times, it is there for entertainment purposes ... no really! They weren't trying to do a documentary! All kidding aside, this can be viewed as fun - no matter what your political background is.\"\n",
    "rating_test=7\n",
    "X_test_tfidf = vectorizer.transform([review_test])\n",
    "predicted_rating = regressor.predict(X_test_tfidf)\n",
    "\n",
    "print(f\"Predicted: {'{:.2f}'.format(predicted_rating[0])} of {rating_test}\")\n",
    "\n",
    "# SIn procesar Predicted: 2.89 of 7\n",
    "# PRocesar Predicted: 2.79 of 7\n",
    "# mod1: Predicted: 4.65 of 7\n",
    "#mod2: Predicted: 3.80 of 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84eef4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "309c9d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 3 of 1\n",
      "Predicted: 6 of 4\n",
      "Predicted: 4 of 1\n",
      "Predicted: 6 of 8\n",
      "Predicted: 8 of 10\n",
      "Predicted: 7 of 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Just content\\nPredicted: 4 of 1\\nPredicted: 6 of 4\\nPredicted: 4 of 1\\nPredicted: 6 of 8\\nPredicted: 9 of 10\\nPredicted: 7 of 9\\n\\nv2\\nPredicted: 3 of 1\\nPredicted: 6 of 4\\nPredicted: 4 of 1\\nPredicted: 5 of 8\\nPredicted: 8 of 10\\nPredicted: 6 of 9\\n\\nPreprocessed:\\nPredicted: 2 of 1\\nPredicted: 6 of 4\\nPredicted: 4 of 1\\nPredicted: 6 of 8\\nPredicted: 8 of 10\\nPredicted: 6 of 9\\n\\n#modified TDIFtokenize\\nPredicted: 5 of 1\\nPredicted: 5 of 4\\nPredicted: 3 of 1\\nPredicted: 4 of 8\\nPredicted: 7 of 10\\nPredicted: 6 of 9\\n\\n# MOD 2\\nPredicted: 3 of 1\\nPredicted: 6 of 4\\nPredicted: 4 of 1\\nPredicted: 4 of 8\\nPredicted: 9 of 10\\nPredicted: 6 of 9\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_reviews=[\n",
    "    \"Eugenics is the study and practice of selective breeding applied to humans, with the aim of improving the species. In a historical and broader sense, eugenics can also be a study of \\\"improving human genetic qualities. \\\"Advocates of eugenics sought to counter what they regarded as spoiler of genetics dynamics within the human gene pool, specifically in regard to congenital disorders and factors relating to the heritability of IQ. This movie is a clear eugenics message. That's to say Eugencists are Striking Back. And Eugenics is totally unethical, and leads to racism.\",\n",
    "    \"\"\"I find it a bit ironic that a movie about the dumbing down of America sums up the message in the first few minutes and then spends nearly 90 minutes repeating the same jokes to get the point across. The message is a bit frighteningly on point in a time when, what seem to be, complete idiots on social media make millions. And I'm guessing many of us have known families like both that were portrayed at the start. I think those two things gain this movie a lot of its accolades despite the 'story'. It had its moments here and there, but as a whole it wasn't as great as the hype.\"\"\",\n",
    "    \"\"\"The story of the movie is just perfect. All futuristic movies describe an advanced civilization. This movie chooses the other way around, outsmarting all other super-advanced-Hollywood-fictional-hi-tech scripts. May be movie was supposed to be rated with \"R+\"; the emphasis on sex-related \"humor\" is off the charts. In my opinion, the prime target for the movie is below 14 years of age, like may be 12. It definitely doesn't address for 25+ people. It's not funny at all, I don't remember I have laughed even once. Story is short; he solves exactly one problem. I think this story could be expressed in far better ways than this movie. It's a brilliant idea, but bad screenplay. Movie tried to tell \"how stupid can we get\" in a stupid (or, may be, \"poor\") way. From this point of view, it's successful; it's all about stupid stuff, and it's all told in stupid way. I, however, still believe that it could be a movie that still shows \"how stupid can we get\" in a smarter way. As a conclusion; it's recommended to be watched but don't expect real comedy or smart jokes. Don't try to watch during your romantic moments, not to mention keep this movie away from your children.\"\"\",\n",
    "    \"\"\"I hate indian movies they're garbage just terrible I mean seriously all these singing, dancing and all the dramatic crying they all look the same to me BUT this is the only movie which I liked original screenplay absolutely flawless movie by a genius director I still can't believe this is indian movie highly recommended for horror genre fans\"\"\",\n",
    "    \"\"\"Tumbbad is not a common movie. It will leave you gripped to your seat making you think \"what would happen next\". Whole movie is so beautifully shot no wonder it is shot in natural light and the beauty is visible. This movie is a feast for your eyes with cinematic excellence is on full display. You wouldnt even have time to wander off your mind but watch it with eyes wide open. You won't enjoy it on small screen but have to watch in cinema halls. Mind you this movie took 6 years in making can you believe in the perseverance of the people who clung onto movie for 6 years despite harships.\"\"\",\n",
    "    \"\"\"Tumbbad is IMO the best Indian horror movie ever, with a strong script and incredibly good shadowy visuals and great sound design (although I will cut a point for the loud RGV-esque background score). Focused direction and a solid lead performance from Sohum Shah (who also produced the film).\"\"\"\n",
    "]\n",
    "_ratings=[\n",
    "    1,\n",
    "    4,\n",
    "    1,\n",
    "    8,\n",
    "    10,\n",
    "    9,\n",
    "]\n",
    "for review_test,rating_test in zip(_reviews,_ratings):\n",
    "    X_test_tfidf = vectorizer.transform([review_test])\n",
    "    predicted_rating = regressor.predict(X_test_tfidf)\n",
    "#     print(f\"Predicted: {'{:.2f}'.format(predicted_rating[0])} of {rating_test}\")\n",
    "    print(f\"Predicted: {math.floor(predicted_rating[0])} of {rating_test}\")\n",
    "    \n",
    "    \n",
    "\"\"\"Just content\n",
    "Predicted: 4 of 1\n",
    "Predicted: 6 of 4\n",
    "Predicted: 4 of 1\n",
    "Predicted: 6 of 8\n",
    "Predicted: 9 of 10\n",
    "Predicted: 7 of 9\n",
    "\n",
    "v2\n",
    "Predicted: 3 of 1\n",
    "Predicted: 6 of 4\n",
    "Predicted: 4 of 1\n",
    "Predicted: 5 of 8\n",
    "Predicted: 8 of 10\n",
    "Predicted: 6 of 9\n",
    "\n",
    "Preprocessed:\n",
    "Predicted: 2 of 1\n",
    "Predicted: 6 of 4\n",
    "Predicted: 4 of 1\n",
    "Predicted: 6 of 8\n",
    "Predicted: 8 of 10\n",
    "Predicted: 6 of 9\n",
    "\n",
    "#modified TDIFtokenize\n",
    "Predicted: 5 of 1\n",
    "Predicted: 5 of 4\n",
    "Predicted: 3 of 1\n",
    "Predicted: 4 of 8\n",
    "Predicted: 7 of 10\n",
    "Predicted: 6 of 9\n",
    "\n",
    "# MOD 2\n",
    "Predicted: 3 of 1\n",
    "Predicted: 6 of 4\n",
    "Predicted: 4 of 1\n",
    "Predicted: 4 of 8\n",
    "Predicted: 9 of 10\n",
    "Predicted: 6 of 9\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85d08e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 9 of 9\n",
      "Predicted: 1 of 1\n",
      "Predicted: 6 of 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sin procesar:\\nPredicted: 9 of 9\\nPredicted: 2 of 1\\nPredicted: 6 of 10\\n\\nProcesando:\\nPredicted: 9 of 9\\nPredicted: 2 of 1\\nPredicted: 6 of 10\\n\\nModTTDIF\\nPredicted: 9 of 9\\nPredicted: 1 of 1\\nPredicted: 6 of 10\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PERSONAL Reviews\n",
    "_reviews=[\n",
    "\"This is one of my most favorite movies, I have enjoyed every aspect of it, from camera, script, effects, and music. All is perfect\",\n",
    "    \"This was a waste of time; Everything was not good, music, dialogue, characters, screeplany. I wish I have spent my time better\",\n",
    "    \"I am sad I haven't knew about this movie before. It has everything on it, not just for a nische, but for everyone.\"\n",
    "]\n",
    "_ratings=[\n",
    "    9,\n",
    "    1,\n",
    "    10,\n",
    "]\n",
    "for review_test,rating_test in zip(_reviews,_ratings):\n",
    "    X_test_tfidf = vectorizer.transform([review_test])\n",
    "    predicted_rating = regressor.predict(X_test_tfidf)\n",
    "#     print(f\"Predicted: {'{:.2f}'.format(predicted_rating[0])} of {rating_test}\")\n",
    "    print(f\"Predicted: {math.floor(predicted_rating[0])} of {rating_test}\")\n",
    "\n",
    "\"\"\"Sin procesar:\n",
    "Predicted: 9 of 9\n",
    "Predicted: 2 of 1\n",
    "Predicted: 6 of 10\n",
    "\n",
    "Procesando:\n",
    "Predicted: 9 of 9\n",
    "Predicted: 2 of 1\n",
    "Predicted: 6 of 10\n",
    "\n",
    "ModTTDIF\n",
    "Predicted: 9 of 9\n",
    "Predicted: 1 of 1\n",
    "Predicted: 6 of 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "99d46bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 4 of 9\n",
      "Predicted: 3 of 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPreProcesado:\\nPredicted: 4 of 9\\nPredicted: 3 of 3'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More reviews\n",
    "_reviews=[\"\"\"This show has a beginning and an actual ending. Not every show can offer that. This TV show is underrated and that's a shame. People need to watch this!\"\"\",\n",
    "         \"\"\"Replicating the 90s movie was tough, I acknowledge that.\n",
    "However, one thing stroke me and I won't be watching any more episodes (currently at s1e10): They find Pieters (the virus' creator) in the container, and there is no mention-not even a thought- about asking him about the cure or having him work on a vaccine.\n",
    "She was point-blank ready to shoot him, I mean, that was the point where it became obvious that the whole series had been designed to draaaag and draaaag and draaaag. Absolutely no credibility.\n",
    "What I would expect from a 12 monkeys tv series after so many years, would be to have the opportunity to dive into the whole 12 monkeys dystopian universe, learn more about the characters and a bit more details about the twists in the original motion picture.\n",
    "Really loving the effort from the cast. From a director's stand-point, its not awesome but its not bad. But in terms of script, the whole thing is too messed up, overly complicated, overly paced and really why they didn't ask Pieters the cure or have him work on the vaccine?\n",
    "Seriously, I wish the creators of Dark had gotten their hands on the script and the realization as a whole.\"\"\",\n",
    "         \n",
    "         ]\n",
    "_ratings=[9,\n",
    "         3]\n",
    "\n",
    "for review_test,rating_test in zip(_reviews,_ratings):\n",
    "    X_test_tfidf = vectorizer.transform([review_test])\n",
    "    predicted_rating = regressor.predict(X_test_tfidf)\n",
    "    print(f\"Predicted: {math.floor(predicted_rating[0])} of {rating_test}\")\n",
    "\n",
    "\"\"\"\n",
    "PreProcesado:\n",
    "Predicted: 4 of 9\n",
    "Predicted: 3 of 3\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb9974",
   "metadata": {},
   "source": [
    "# METHOD 2 \n",
    "# Embedding: word2vec  + Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f4df658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "# from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c51e4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# for w in text:\n",
    "#             if w.isalnum():\n",
    "#                 _words.append(w if not lower else w.lower())\n",
    "\n",
    "\n",
    "# for r in df['rating']\n",
    "\n",
    "# df['content_ed']=[  (z for z in x if z.isalnum())   for x in [word_tokenize(y) for y in df['content']] ]\n",
    "\n",
    "contents=[]\n",
    "for c in df['content']:\n",
    "    tokens=[]\n",
    "    for t in word_tokenize(c):\n",
    "        if t not in stop_words and t.isalnum():\n",
    "            tokens.append(t)\n",
    "    contents.append(tokens)\n",
    "df['content_ed']=contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80256739",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'meritocracy,' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert each review into a sequence of word embeddings\u001b[39;00m\n\u001b[1;32m     13\u001b[0m X_train_sequences \u001b[38;5;241m=\u001b[39m [[word2vec_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m review\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m X_train]\n\u001b[0;32m---> 14\u001b[0m X_test_sequences \u001b[38;5;241m=\u001b[39m [[word2vec_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m review\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m X_test] \u001b[38;5;66;03m#<Error\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Pad the sequences to a fixed length\u001b[39;00m\n\u001b[1;32m     17\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sequence) \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m X_train_sequences)\n",
      "Cell \u001b[0;32mIn[64], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert each review into a sequence of word embeddings\u001b[39;00m\n\u001b[1;32m     13\u001b[0m X_train_sequences \u001b[38;5;241m=\u001b[39m [[word2vec_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m review\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m X_train]\n\u001b[0;32m---> 14\u001b[0m X_test_sequences \u001b[38;5;241m=\u001b[39m [[word2vec_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m review\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m X_test] \u001b[38;5;66;03m#<Error\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Pad the sequences to a fixed length\u001b[39;00m\n\u001b[1;32m     17\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sequence) \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m X_train_sequences)\n",
      "Cell \u001b[0;32mIn[64], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert each review into a sequence of word embeddings\u001b[39;00m\n\u001b[1;32m     13\u001b[0m X_train_sequences \u001b[38;5;241m=\u001b[39m [[word2vec_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m review\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m X_train]\n\u001b[0;32m---> 14\u001b[0m X_test_sequences \u001b[38;5;241m=\u001b[39m [[\u001b[43mword2vec_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m review\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m X_test] \u001b[38;5;66;03m#<Error\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Pad the sequences to a fixed length\u001b[39;00m\n\u001b[1;32m     17\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sequence) \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m X_train_sequences)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sem2/lib/python3.9/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sem2/lib/python3.9/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sem2/lib/python3.9/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'meritocracy,' not present\""
     ]
    }
   ],
   "source": [
    "# Assuming you have a list of movie reviews and their corresponding ratings\n",
    "reviews = df['content']\n",
    "ratings = df['rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and train Word2Vec on the training reviews\n",
    "tokenized_reviews = [review.split() for review in X_train]\n",
    "word2vec_model = Word2Vec(tokenized_reviews, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Convert each review into a sequence of word embeddings\n",
    "X_train_sequences = [[word2vec_model.wv[word] for word in review.split()] for review in X_train]\n",
    "X_test_sequences = [[word2vec_model.wv[word] for word in review.split()] for review in X_test] #<Error\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_sequence_length = max(len(sequence) for sequence in X_train_sequences)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create a neural network model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(max_sequence_length, word2vec_model.vector_size)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Predict ratings for the testing reviews\n",
    "y_pred = model.predict(X_test_padded).flatten()\n",
    "\n",
    "# Evaluate the model using mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b7e541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found on  ['In', 'meritocracy', 'success', 'fortune', 'reserved', 'deserve', 'develop', 'solid', 'plans', 'according', 'talents', 'abilities', 'execute', 'plans', 'hard', 'work', 'determination', 'Anyone', 'rise', 'top', 'lucky', 'Cinderella', 'plucked', 'cinders', 'gussied', 'gowns', 'meritocracy', 'represents', 'heights', 'perfect', 'egalitarian', 'society', 'I', 'started', 'nothing', 'ended', 'everything', 'I', 'ever', 'desired', 'achieve', 'dreams', 'try', 'The', 'promise', 'unobstructed', 'sunshine', 'top', 'mountain', 'becomes', 'justification', 'bitter', 'competition', 'backstabbing', 'deceit', 'callousness', 'You', 'climb', 'crooked', 'ladder', 'make', 'straight', 'one', 'perhaps', 'last', 'feel', 'secure', 'afford', 'kind', 'confident', 'generous', 'It', 'easy', 'nice', 'rich', 'mother', 'film', 'Jang', 'one', 'point', 'long', 'crooked', 'ladder', 'sometimes', 'rungs', 'give', 'beneath', 'grip', 'sometimes', 'dangerously', 'greased', 'climbed', 'sometimes', 'ladder', 'simply', 'kicked', 'either', 'often', 'staring', 'ground', 'There', 'lot', 'people', 'trying', 'climb', 'one', 'meritocracy', 'ca', 'blame', 'ladder', 'people', 'trying', 'climb', 'Nor', 'blame', 'fact', 'good', 'stuff', 'kept', 'many', 'stories', 'instead', 'ground', 'everyone', 'easily', 'reach', 'No', 'must', 'blame', 'You', 'tread', 'carefully', 'You', 'climbed', 'quickly', 'You', 'used', 'firmer', 'precise', 'grip', 'anticipated', 'disasters', 'known', 'leap', 'If', 'fail', 'meritocracy', 'fault', 'You', 'tried', 'harder', 'Better', 'luck', 'next', 'Choi', 'young', 'man', 'main', 'character', 'PARASITE', 'several', 'times', 'refers', 'metaphors', 'film', 'course', 'metaphor', 'On', 'surface', 'level', 'viewers', 'treated', 'thrilling', 'engaging', 'crime', 'story', 'At', 'times', 'however', 'bubbling', 'beneath', 'slick', 'surface', 'genre', 'film', 'deeply', 'personal', 'meaningful', 'truths', 'resonate', 'almost', 'viewer', 'These', 'insights', 'rarely', 'foregrounded', 'They', 'subtly', 'interwoven', 'fact', 'like', 'may', 'completely', 'surprised', 'final', 'shots', 'film', 'roll', 'realize', 'emotionally', 'devastated', 'intimate', 'humanist', 'story', 'witnessed', 'Bong', 'filmmaking', 'extraordinary', 'make', 'fully', 'invested', 'lives', 'characters', 'without', 'even', 'realizing', 'done', 'want', 'avoid', 'spoilers', 'suffice', 'say', 'PARASITE', 'masterpiece', 'beautifully', 'lensed', 'enthrallingly', 'edited', 'superbly', 'acted', 'intimately', 'Korea', 'population', 'one', 'sixth', 'size', 'United', 'States', 'population', 'stacked', 'skyscrapers', 'area', 'slightly', 'smaller', 'state', 'Kentucky', 'Higher', 'education', 'widespread', 'parents', 'means', 'try', 'make', 'children', 'stand', 'pack', 'hiring', 'tutors', 'signing', 'extracurriculars', 'afterschool', 'programs', 'I', 'lived', 'Korea', 'children', 'I', 'taught', 'sometimes', 'engaged', 'learning', 'ten', 'twelve', 'hours', 'day', 'six', 'days', 'week', 'public', 'school', 'private', 'school', 'piano', 'class', 'soccer', 'team', 'taekwondo', 'math', 'camp', 'chess', 'club', 'I', 'routinely', 'worked', 'sixty', 'seventy', 'hours', 'week', 'salary', 'bars', 'I', 'would', 'meet', 'young', 'men', 'age', 'expected', 'work', 'far', 'slept', 'desks', 'need', 'pry', 'work', 'long', 'As', 'father', 'Song', 'film', 'one', 'point', 'says', 'country', 'fifty', 'young', 'men', 'college', 'degrees', 'apply', 'mere', 'security', 'guard', 'job', 'One', 'ca', 'afford', 'themes', 'story', 'localized', 'Korea', 'however', 'They', 'story', 'global', 'capitalism', 'specter', 'American', 'materialism', 'imperialism', 'note', 'Indians', 'looms', 'heavily', 'film', 'Meritocracy', 'makes', 'cannibals', 'us', 'It', 'nice', 'dream', 'sometimes', 'dreamers', 'plan', 'struggle', 'well', 'enough', 'indeed', 'climb', 'basement', 'sunshine', 'nice', 'ending', 'But', 'film', 'also', 'makes', 'clear', 'sometimes', 'planning', 'dreaming', 'may', 'maybe', 'whims', 'fancy', 'More', 'often', 'seems', 'pipe', 'dreams', 'content', 'leave', 'us', 'nothing', 'whiff', 'spewed', 'sewage']\n",
      "741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title                           Meritocracy: it\\'s metaphorical\n",
       "content       In a meritocracy, success and fortune are rese...\n",
       "rating                                                       10\n",
       "content_ed    [In, meritocracy, success, fortune, reserved, ...\n",
       "Name: 852, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=0\n",
    "for i,t in enumerate(df['content_ed']):\n",
    "    if('meritocracy' in t):\n",
    "        print('found on ', t)\n",
    "        index=i\n",
    "print(index)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9276398b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                           Meritocracy: it\\'s metaphorical\n",
       "content       In a meritocracy, success and fortune are rese...\n",
       "rating                                                       10\n",
       "content_ed    [In, meritocracy, success, fortune, reserved, ...\n",
       "Name: 852, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc222911",
   "metadata": {},
   "source": [
    "# Method 3\n",
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "389fa022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivan/opt/anaconda3/envs/sem2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 5.58MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.0/28.0 [00:00<00:00, 9.90kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 182kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440M/440M [00:16<00:00, 26.0MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Tokenize and encode the reviews\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m X_train_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m X_test_encoded \u001b[38;5;241m=\u001b[39m tokenizer(X_test, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Obtain the BERT embeddings for the reviews\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sem2/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2488\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2487\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2488\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sem2/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2546\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2543\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2546\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2547\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2548\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2549\u001b[0m     )\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have a list of movie reviews and their corresponding ratings\n",
    "reviews = df['content']\n",
    "ratings = df['rating']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the reviews\n",
    "X_train_encoded = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt') #ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
    "X_test_encoded = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Obtain the BERT embeddings for the reviews\n",
    "with torch.no_grad():\n",
    "    X_train_embeddings = model(**X_train_encoded).last_hidden_state.mean(dim=1)\n",
    "    X_test_embeddings = model(**X_test_encoded).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Create a simple feed-forward neural network\n",
    "class RatingPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RatingPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert data to tensors and move to device\n",
    "X_train_tensors = torch.tensor(X_train_embeddings).to(device)\n",
    "y_train_tensors = torch.tensor(y_train).unsqueeze(1).to(device)\n",
    "X_test_tensors = torch.tensor(X_test_embeddings).to(device)\n",
    "\n",
    "# Initialize the model and move to device\n",
    "model = RatingPredictor(X_train_tensors.size(1)).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, X_train_tensors.size(0), batch_size):\n",
    "        inputs = X_train_tensors[i:i+batch_size]\n",
    "        labels = y_train_tensors[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / X_train_tensors.size(0)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensors).squeeze().cpu().numpy()\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520c1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
